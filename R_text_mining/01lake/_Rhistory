which(rownames(tdm) == x))
topID = unlist(topID)
plot_ly(data = as.data.frame(doc.tfidf),
x = as.numeric(colnames(doc.tfidf)),
y = doc.tfidf[topID[10],],
name = rownames(doc.tfidf)[topID[10]],
type = "scatter", mode= "box") %>%
add_trace(y = doc.tfidf[topID[2],],
name = rownames(doc.tfidf)[topID[2]])
# get short doc matrix
nonzero = (doc.tfidf != rep(0,10))
nonzeroid = which(row_sums(nonzero) != 0)
q <- rownames(doc.tfidf[nonzeroid,])
all.term <- rownames(doc.tfidf)
loc <- which(all.term %in% q)
s.tdm <- doc.tfidf[loc,]
View(s.tdm)
# result : cos similarity ranking
cos.sim <- function(x, y)
{
(as.vector(x) %*% as.vector(y)) / (norm(as.matrix(x)) * norm(y))
}
doc.cos <- cos.sim(x=as.matrix(s.tdm[,1]),
y=as.matrix(s.tdm[,2]))
doc.cos <- apply(s.tdm[,2:10], 2, cos.sim,
y=as.matrix(s.tdm[,2]))
orderDoc <- doc.cos[order(doc.cos, decreasing = TRUE)]
plot_ly(data = as.data.frame(orderDoc),
x = rownames(as.data.frame(orderDoc)),
y = orderDoc,
name = rownames(doc.tfidf)[topID[10]],
type = "bar", mode= "box")
# Kmeans 分群
library(stats)
kmeansOut <- kmeans(doc.tfidf, 5, nstart = 20)
library(devtools)
library(ggbiplot)
testTfidf = doc.tfidf
tfidf.pca <- prcomp(testTfidf)
tfidf.kmeans <- as.factor(kmeansOut$cluster)
g <- ggbiplot(tfidf.pca, obs.scale = 1, var.scale = 1,
groups = tfidf.kmeans, ellipse = TRUE,
circle = TRUE, labels = rownames(testTfidf))
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
# tf-idf computation #try use dataframe
#tdm<-tdm$dimnames %in% roles
#class(tdm)
#extract_f<-tdm%>%
#  as.matrix()%>%
#  as.data.frame()
#extract_f$names<-rownames(extract_f)
#extract_t<-filter(extract_f,names  %in% roles)
#extract_t$names<-NULL
#extract_t
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
roles = readLines("主角名單1.txt", encoding="UTF-8")
new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:9, ]))
v <- as.vector(tdm$dimnames$Terms %in% roles)
tdm<- tdm[v, ]
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
# 可以用 findFreqTerms
# 看看在所有文件裡出現 200 次以上的關鍵字有哪些。
result = findFreqTerms(tdm,200)
result
ass = findAssocs(tdm, "宋江", 0.1)
ass
# 畫出 tf-idf 統計圖
library(plotly)
topID = lapply(rownames(as.data.frame(ass)), function(x)
which(rownames(tdm) == x))
topID = unlist(topID)
plot_ly(data = as.data.frame(doc.tfidf),
x = as.numeric(colnames(doc.tfidf)),
y = doc.tfidf[topID[10],],
name = rownames(doc.tfidf)[topID[10]],
type = "scatter", mode= "box") %>%
add_trace(y = doc.tfidf[topID[2],],
name = rownames(doc.tfidf)[topID[2]])
# get short doc matrix
nonzero = (doc.tfidf != rep(0,10))
nonzeroid = which(row_sums(nonzero) != 0)
q <- rownames(doc.tfidf[nonzeroid,])
all.term <- rownames(doc.tfidf)
loc <- which(all.term %in% q)
s.tdm <- doc.tfidf[loc,]
View(s.tdm)
# result : cos similarity ranking
cos.sim <- function(x, y)
{
(as.vector(x) %*% as.vector(y)) / (norm(as.matrix(x)) * norm(y))
}
doc.cos <- cos.sim(x=as.matrix(s.tdm[,1]),
y=as.matrix(s.tdm[,2]))
doc.cos <- apply(s.tdm[,2:10], 2, cos.sim,
y=as.matrix(s.tdm[,2]))
orderDoc <- doc.cos[order(doc.cos, decreasing = TRUE)]
plot_ly(data = as.data.frame(orderDoc),
x = rownames(as.data.frame(orderDoc)),
y = orderDoc,
name = rownames(doc.tfidf)[topID[10]],
type = "bar", mode= "box")
# Kmeans 分群
library(stats)
kmeansOut <- kmeans(doc.tfidf, 5, nstart = 20)
library(devtools)
library(ggbiplot)
testTfidf = doc.tfidf
tfidf.pca <- prcomp(testTfidf)
tfidf.kmeans <- as.factor(kmeansOut$cluster)
g <- ggbiplot(tfidf.pca, obs.scale = 1, var.scale = 1,
groups = tfidf.kmeans, ellipse = TRUE,
circle = TRUE, labels = rownames(testTfidf))
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
# tf-idf computation #try use dataframe
#tdm<-tdm$dimnames %in% roles
#class(tdm)
#extract_f<-tdm%>%
#  as.matrix()%>%
#  as.data.frame()
#extract_f$names<-rownames(extract_f)
#extract_t<-filter(extract_f,names  %in% roles)
#extract_t$names<-NULL
#extract_t
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
roles = readLines("主角名單1.txt", encoding="UTF-8")
new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:500, ]))
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
roles = readLines("主角名單1.txt", encoding="UTF-8")
new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:500, ]))
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
roles = readLines("主角名單1.txt", encoding="UTF-8")
new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:500, ]))
roles
length(roles)
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
roles = readLines("主角名單1.txt", encoding="UTF-8")
new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:500, ]))
View(inspect(tdm[1:26, ]))
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
#mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
mixseg = worker()
roles = readLines("主角名單1.txt", encoding="UTF-8")
#new_user_word(mixseg, unlist(roles), rep("n", length(unlist(roles))))
#cutter = worker(bylines = TRUE, stop_word = "stop.txt")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
View(inspect(tdm[1:500, ]))
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, toSpace, "◆")
docs <- tm_map(docs, toSpace, "‧")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "阿")
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#移除標點符號 (punctuation)
#移除數字 (digits)、空白 (white space)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#View(inspect(docs[2]))
#利用jiebaR去除
#mixseg = worker(bylines = TRUE, stop_word = "stop.txt")
mixseg = worker()
roles = readLines("主角名單1.txt", encoding="UTF-8")
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus,
control = list(wordLengths = c(2, Inf)))
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
library(showtext)
library(ggplot2)
library(tm)
library(jiebaRD)
library(jiebaR)
#Sys.setenv(JAVA_HOME="C:/Program Files/Java/dk-9.0.1")
#library(rJava)
library(SnowballC)
library(slam)
library(Matrix)
data.path<-"E:/R/R_text_mining/01lake/"
setwd(data.path)
#read files from txt 讀取水滸傳120回
filenames <- list.files(paste0(getwd(),"/source"), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
files <- lapply(filenames, readLines)
files <- lapply(filenames, readLines)
files <- lapply(filenames, readLines)
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
